   过去的近一年我主要参与的项目都是基于react的多页应用——苏打办公，一款以pdf和办公相关功能为核心的软件，参与了原有项目的改造和迁移，也上线了不少需求。其中也遇到了不少问题。

## 1.项目的迁移

​    背景: 原有项目是一个相对复杂的多页应用，采用了React和Webpack作为构建工具。然而，随着项目的发展，我们发现存在一些问题。首先，项目中的页面结构较为混乱，大多数功能页面都是以主页面的子路由方式拆分的，导致代码结构难以维护和理解。其次，Webpack构建时间过长，严重影响了开发效率。为了解决这些问题，我们决定将项目的构建工具迁移到Vite，并将原有的子路由重构为独立的页面。

1. 初期迁移过程中，我们遇到了一些挑战。首先，我们在迁移时选择了使用基于history路由，但忽略了原项目基于hash路由的特点。这导致在服务器上没有对history路由进行兼容处理，结果是直接跳转到某个子路由时会出现白屏问题。

2. 由于原项目规模较大，重构涉及到许多逻辑变更，因此开发时间较长。然而，我们在任务拆解方面没有做足够的准备，导致一次提交可能包含大量代码修改。当遇到代码冲突或其他问题时，很难迅速定位问题的根本原因。

3. 我和同事虽然开发功能模块不一样，但是有一部分有较强的耦合性，我们在开发时却写了两套代码。

**项目教训和改进点：**

1. **兼容性问题的重要性：** 项目初期遇到的兼容性问题表明，在进行技术迁移时，不可忽视原有架构的兼容性。为了避免未来的问题，我们需要更系统地分析项目依赖，并对兼容性问题有更深入的了解。
2. **任务拆解和规划：** 项目中，我们没有充分规划和拆解任务，导致一次性提交包含了大量代码修改。下次类似的项目，我们计划更精细化，将任务细分为小单元，以降低代码冲突和问题排查的复杂度。
3. **沟通和协作：** 在多人进行的协作开发时，我们应该和同事多进行沟通交流，对各自的任务有明确的划分，发现是否有可以直接复用的逻辑，加快开发速度。

## 2.如何合理的处理流式响应

  背景:前端实现打字机效果，需对后端返回的流进行改造，拼接，但是产品。
        传统的请求方式主要依赖于使用Ajax来向后端发起请求，这种方式有一个明显的特点：一次请求只能接收一次响应数据，请求完成后即结束。然而，在某些场景下，我们需要持续地接收来自后端的数据更新，这时就需要摒弃传统的Ajax方式，而选择一些更适合实现实时数据更新的方法。
      然后调研了业界主流的处理方案，最终关注到了两个主要的API：EventSource和Fetch，它们分别具有不同的优势和用途。于是我们分别对两个api做了对比。

| 特性     | EventSource                                                  | fetch API                                                    |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 兼容性   | 广泛支持，包括Internet Explorer 8及更高版本                  | 在较新的浏览器中得到支持，不完全支持Internet Explorer        |
| 数据格式 | 只支持服务器发送的文本数据，自动转换为文本                   | 可以获取包括文本、JSON、Blob等在内的各种数据格式             |
| 错误处理 | 自动尝试重新连接，可以监听'error'事件来处理错误              | 没有内置的重试机制，需要手动处理错误并可能需要进行重试       |
| 流式处理 | 支持简单处理服务器发送的流式数据                             | 不直接支持流式处理，但可以使用Response对象的body属性获取流式接口 |
| CORS问题 | 受同源策略限制，除非服务器配置了适当的CORS头，否则无法跨源加载 | 不受同源策略限制，可以跨源请求数据，但需要服务器配置适当的CORS头 |
| 灵活性   | 只能发送GET请求，拼接字符串传参                              | 可以发起任意类型请求。传参灵活                               |

​         于是最后采用的fetch来作为解决方案，但是这只是解决了第一个问题。然后就是fetch读取的数据，具体实现中也会遇到很多问题。下面是示例代码。

```javascript
后端返回的数据结构
//data: {"content":" \n\n"}

//data: {"content":"克服"}

//data: {"content":"拖延"}

//data: {"content":"症"}

//data: {"content":"?"}
//null 结尾
async function fetchDataAndDisplay() {
  const url = 'your-backend-url'; 
  const response = await fetch(url, { stream: true });
  const reader = response.body.getReader();

  let outputElement = document.getElementById('output');
  let text = '';

  while (reader) {
    try {
      const { value, done } = await reader.read()
      const chars = new TextDecoder().decode(value)
      if (chunk.value === '' ||done) {
        break; 
      }
      const dataArray = chars.trim().split('\n\n')
      const jsonObjects = dataArray.map(data => {
        if (data === 'null' || !data) {
          return { content: '' }
        }
        const jsonString = data.substring('data: '.length)
        return JSON.parse(jsonString) as { content: string }
      })
      let text=''
      jsonObjects.forEach(item => {
        text += item.content
      })
      console.log('text',text)
    } catch (err) {
      console.error(err);
    }
  }
}
```

​        首先，我们需要将返回的unit8Array转换为中文字符串。

​        然后，我们可以通过正则表达式来匹配出每个数据结构中content的value值。但是，这种方法可能会比较复杂，因为我们需要先解析出整个数据结构，然后再进行匹配。此外，正则表达式也可能因为数据结构的复杂性而变得难以维护。

​        另一种方法是使用JSON.parse将unit8Array转换为JSON对象。这种方法的好处是对象的属性操作和读取都更加方便。但是，如果使用不当，JSON.parse很容易抛出错误，导致程序中断。因此，在处理时需要特别小心。

​       同时，后端返回的数据格式也有不可控的情况。因此，我们需要对数据进行检查和处理，以确保程序的稳定性和可靠性。

**复盘和改进点：**

**错误处理：** 本次的的需求其实是拥有很长的请求链路，从上传文件到最后获取结果，其中的可能会出现错误异常情况较多。我们需要对常见的错误处理有所了解，保证代码的健壮性。

**方案调研：**我们在开发中难免会遇到没有使用过的技术，在开发之前，如果市面上有类似的竞品，我们可以参考其设计思路，避免开发过程中的弯路。

## 3.从0到一的项目开发

​      360看图项目的前期搭建是由我个人负责的。原始的功能是完全由客户端实现的，但现在需要对部分功能进行Web化改造，以便后续维护。为了便于维护和将不同功能进行拆解，我们决定继续采用MPA架构。考虑到团队的技术栈仍然使用React和Vite，来实现最大化提升工作效率。

​      首个需求是图片转文字功能。客户端需要将获取到的图片传递给前端，然后前端负责展示和上传。然而，客户端只能通过Base64编码将图片传递给前端，而对于较大的图片，必须进行分片传输，以确保数据完整性。在分片传输过程中，每个数据块都被编号，以验证最终数据的完整性。但是，Base64编码有一个明显的缺陷，它会增大文件体积。在调试过程中，我们发现如果以Base64格式表示的图片超过20MB，很容易导致前端页面崩溃或卡死。因此，我们需要进行一些优化。

​       我们的优化思路是不直接将超长的Base64数据渲染到页面上，而是将Base64数据转化为Blob对象，然后通过Blob生成一个可访问的URL。这样做的好处是可以减小页面负担，提高性能，并且减少了页面卡死的风险。示例如下：

```javascript
const base64Data = "yourBase64DataHere"; // 接收的base64数据

const blobToImageUrl = (blobData, chunkSize) => {
  // 声明一个变量来保存当前已读取的数据的位置
  let currentPosition = 0;
  // 声明一个变量来保存当前已读取的 Blob 数据
  let accumulatedBlob = null;
  const chunk = blobData.slice(
    currentPosition,
    currentPosition + chunkSize
  );

  // 判断是否还有数据未读取
  while (currentPosition < fullBlob.size) {
    // 如果是第一次读取，直接赋值给 accumulatedBlob
    if (!accumulatedBlob) {
      accumulatedBlob = chunk;
    } else {
      // 否则，拼接到已有数据后面
      accumulatedBlob = new Blob([accumulatedBlob, chunk]);
    }
    // 更新当前位置
    currentPosition += chunk.size;
  }
  // 已经读取完整个 Blob，将其转换为图像 URL
  const imageUrl = URL.createObjectURL(accumulatedBlob);
  return imageUrl;
};
//通过工具函数实现base64到blob的转换
const blob = base64ToBlob(base64Data, contentType);
const imageUrl = blobToImageUrl(blob);

```

​        这个优化策略，不仅提高了性能，还保证了数据的完整性，同时改善了用户体验。如果通过图片压缩和延迟加载等更多手段，还可以进一步提升前端页面的性能和响应速度。通过这些改进，我们能够更好地满足360看图项目的需求，并确保项目的可维护性和性能都达到预期水平。
